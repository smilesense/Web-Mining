{"cells":[{"cell_type":"markdown","source":["# Tugas Web Mining\n","\n","Nama : Bihubbil Choir Aidifta<p>\n","NIM : 190411100121"],"metadata":{"id":"f-SEZUEgFYmK"}},{"cell_type":"markdown","source":["## Web Mining\n","\n","Web Mining adalah proses teknik Data Mining untuk secara otomatis menemukan dan mengekstrak informasi dari suatu halaman Web. Tujuan utama penambangan web adalah menemukan informasi yang berguna.\n","\n","Penambangan web membantu meningkatkan kekuatan mesin pencari web dengan mengklasifikasikan dokumen web dan mengidentifikasi halaman web."],"metadata":{"id":"wxtVlTLgVTap"}},{"cell_type":"markdown","metadata":{"id":"UGEttwL5wtBj"},"source":["# Instalasi Library\n","Pada tahap ini dilakukan instalasi library-library yang nantinya dibutuhkan untuk menjalankan program, berikut adalah library yang dibutuhkan : \n","\n","---\n","\n","\n","\n","> ##  Snscrape\n","Snscrape merupakan perpustakaan / library sumber terbuka (open source) yang ditulis dengan bahasa pemrograman python dan berguna untuk melakukan scraping atau ekstraksi dari jejaring sosial. Pada program ini kita akan menggunakan Snscrape untuk mengambil/mengekstrak data Tweets dari Twitter.\n","\n","> ## Pandas\n","Pandas merupakan perpustakaan / library sumber terbuka (Open Source) yang ditulis dengan bahasa pemrograman python dan berguna untuk melakukan analisis dan manipulasi data, khususnya untuk memanipulasi tabel numerik dan deret waktu.\n","Pada program ini kita akan memanfaatkan Pandas DataFrame untuk menempatkan data hasil crawling.\n","\n","> ## Sastrawi\n","Sastrawi merupakan perpustakaan / library sumber terbuka yang ditulis dengan bahasa pemrograman python dan berguna untuk melakukan pengurangan kata-kata yang ter- infleksi dalam bahasa Indonesia ke bentuk baku-nya atau sesuai dengan standar kamus.\n","\n","> ## Scikit-learn\n","Scikit-learn adalah library machine learning open source  untuk bahasa pemrograman Python. Ini fitur berbagai klasifikasi, regresi, algoritma pengelompokan, dan alat yang efisien untuk data mining dan analisis data. Ini dibangun di atas NumPy, SciPy, dan Matplotlib.\n"]},{"cell_type":"code","execution_count":133,"metadata":{"id":"w2l3X2eGNHoN","executionInfo":{"status":"ok","timestamp":1664128979842,"user_tz":-420,"elapsed":12803,"user":{"displayName":"Adit Smile","userId":"14198219305169166491"}}},"outputs":[],"source":["%%capture\n","!pip install snscrape\n","!pip install pandas\n","!pip install Sastrawi\n","!pip install scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"L_EImVm8xRYx"},"source":["# Menulis Script Konfigurasi Snscrape\n","Pada tahap ini akan dibuat script kofigurasi Snscrape, script ini sendiri berupa modul yang berisi fungsi-fungsi untuk melakukan crawling dan scraping data tweet yang berada pada Twitter.<p>\n","Berikut adalah penjelasan singkat mengenai script tersebut.\n","\n","---\n","<br>\n","\n","```\n","search_query = \"Jokowi\"\n","jumlah_tweets = 100\n","tweets = []\n","```\n","\n","Variable tersebut merupakan variable yang berguna untuk mendefinisikan \"search query\" yang akan digunakan untuk mencari topik spesifik di twitter, \"jumlah tweet\" yang nantinya akan diambil, serta sebuah wadah berupa \"list\" yang nantinya akan digunakan untuk menyimpan data hasil scrape.\n","\n","---\n","\n","<br>\n","\n","```\n","for tweet in sntwitter.TwitterSearchScraper(search_query).get_items():\n","    \n","    if len(tweets) == jumlah_tweets:\n","        break\n","    else:\n","        tweets.append([tweet.date, tweet.username, tweet.content, 'None'])\n","```\n","\n","Loop ini berfungsi untuk mengambil tiap data tweet yang berhasil diambil dari twitter berdasarkan query yang telah kita definisikan sebelumnya. Pada loop tersebut terdapat pengecekan kondisi yang akan mengecek jumlah tweet yang berhasil diambil, selama loop ini belum mencapai batas jumlah tweet yang kita tentukan sebelumnya, maka loop ini akan terus melakukan penambahan data tweet ke wadah(list) yang telah dibuat sebelumnya.\n","\n","\n","---\n","\n","<br>\n","\n","```\n","df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet', 'Label'])\n","```\n","Variable diatas berfungsi untuk mendefinisikan data tweet yang sebelumnya telah ditempatkan di wadah(list), dalam bentuk Pandas Dataframe.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":134,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4446,"status":"ok","timestamp":1664128984283,"user":{"displayName":"Adit Smile","userId":"14198219305169166491"},"user_tz":-420},"id":"symZrwQqxgcT","outputId":"2b78c1d3-a8f0-43b3-8b82-6c3630b7f326"},"outputs":[{"output_type":"stream","name":"stdout","text":["                        Date             User  \\\n","0  2022-09-25 18:02:28+00:00        Massbboyy   \n","1  2022-09-25 18:01:58+00:00          yo2thok   \n","2  2022-09-25 18:01:50+00:00        den_denai   \n","3  2022-09-25 18:01:23+00:00   Parlindsiregar   \n","4  2022-09-25 18:00:54+00:00        otabibar1   \n","..                       ...              ...   \n","95 2022-09-25 17:27:38+00:00    OposisiCerdas   \n","96 2022-09-25 17:27:24+00:00      Anak_Mama27   \n","97 2022-09-25 17:26:50+00:00   SugiKawuloAlit   \n","98 2022-09-25 17:26:32+00:00  HenryII63936538   \n","99 2022-09-25 17:26:29+00:00       JonieDamns   \n","\n","                                                Tweet Label  \n","0              @50shadesofpadme Inget apa kata jokowi  None  \n","1   Harus diapresiasi pernyataan Prabowo ini sebag...  None  \n","2   @RelawanAnies24 @jokowi Yg demo,orang itu\" aj....  None  \n","3   @I17082000 @reprabo07 @msaid_didu @jokowi Ini ...  None  \n","4   @ChusnulCh__ @ganjarpranowo Manusia titipan tu...  None  \n","..                                                ...   ...  \n","95  Pangi: Serangan AHY ke Jokowi Sekadar untuk Du...  None  \n","96  https://t.co/30Mxm8PXna\\nPT Permodalan Nasiona...  None  \n","97  @galuh_bambang @Saibi_Kasim @msaid_didu Sibuk ...  None  \n","98  @patriot016610 @jokowi @aniesbaswedan https://...  None  \n","99  @Yus73968403 @alisyarief @HusinShihab @jokowi ...  None  \n","\n","[100 rows x 4 columns]\n"]}],"source":["\n","import snscrape.modules.twitter as sntwitter\n","import pandas as pd\n","\n","search_query = \"Jokowi\"\n","jumlah_tweets = 100\n","tweets = []\n","\n","\n","for tweet in sntwitter.TwitterSearchScraper(search_query).get_items():\n","    \n","    if len(tweets) == jumlah_tweets:\n","        break\n","    else:\n","        tweets.append([tweet.date, tweet.username, tweet.content, 'None'])\n","        \n","df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet', 'Label'])\n","print(df)\n"]},{"cell_type":"markdown","source":["## Melakukan Pengecekan Berkas Hasil Scrape Lama\n","\n","Karena setelah melakukan scraping data tweet kita perlu memberikan label secara manual, maka untuk menghindari berkas lama tertimpa oleh berkas baru, disini kita akan melakukan pengecekan berkas hasil scrape lama, apakah ada pada direktori atau tidak. Jika berkas hasil scrape lama tidak ditemukan pada direktori, maka hasil scrape sebelumnya (yang ada pada Pandas Dataframe) akan diespor menjadi berkas csv."],"metadata":{"id":"YMiuGYqDuYEC"}},{"cell_type":"code","source":["%%capture\n","import os\n","!wget https://raw.githubusercontent.com/smilesense/datasets/master/ppw/19_121_tweets_labeled.csv -O 19_121_tweets_labeled.csv\n","\n","\n","output_stream = os.popen('cat 19_121_tweets_labeled.csv')\n","res = output_stream.read()\n","if res == '':\n","  df.to_csv('19_121_tweets_labeled.csv')"],"metadata":{"id":"Z4kZM__8WUY0","executionInfo":{"status":"ok","timestamp":1664128984284,"user_tz":-420,"elapsed":14,"user":{"displayName":"Adit Smile","userId":"14198219305169166491"}}},"execution_count":135,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRrYwIJsyNwG"},"source":["# Import Hasil\n","Setelah perintah sebelumnya berhasil dijalankan, selanjutnya kita akan melakukan import isi berkas hasil scrape sebelumnya kedalam Pandas DataFrame."]},{"cell_type":"code","execution_count":136,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664128984284,"user":{"displayName":"Adit Smile","userId":"14198219305169166491"},"user_tz":-420},"id":"6GwmbpBTyPw4","outputId":"89d082ec-53a5-402b-d8c4-c57ec9f9ad9a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              User                                              Tweet    Label\n","0          Truncvg  @jokowi Mentolo napuk cocot.e samean aku pak\\r...  Negatif\n","1     WartaEkonomi  Achmad Nur Hidayat Ungkap Kemungkinan Kaitan P...  Negatif\n","2      okezonenews  Bjorka: Apa Anda (Jokowi) butuh bantuan saya u...  Negatif\n","3   infoBanyumas12  #Foto Presiden Jokowi mengakui kenaikan harga ...  Positif\n","4          Metdro2  @Yatie84991237 @_TNIAL_ @edhi_shi @JohnVirgoVj...  Negatif\n","..             ...                                                ...      ...\n","76           bn_1c  President Jokowi is Committed to Resolving Hum...  Positif\n","77   adi27setiawan  @jokowi Telek pak2, kekayaan indo hanya dinikm...  Negatif\n","78      AyahFerGie  @yosnggarang @jokowi Tiada hari tanpa omong ko...  Negatif\n","79        de_fatah  @JSuryoP1 Kasihan Pak Prabowo juga. Sudah rela...  Negatif\n","80   mofaisalakbar  @fadjarkim_ Itu orang pssi mirip jokowi, pante...  Negatif\n","\n","[81 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-0fadd632-9aad-439f-9a10-860ea9a3b0c7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User</th>\n","      <th>Tweet</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Truncvg</td>\n","      <td>@jokowi Mentolo napuk cocot.e samean aku pak\\r...</td>\n","      <td>Negatif</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>WartaEkonomi</td>\n","      <td>Achmad Nur Hidayat Ungkap Kemungkinan Kaitan P...</td>\n","      <td>Negatif</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>okezonenews</td>\n","      <td>Bjorka: Apa Anda (Jokowi) butuh bantuan saya u...</td>\n","      <td>Negatif</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>infoBanyumas12</td>\n","      <td>#Foto Presiden Jokowi mengakui kenaikan harga ...</td>\n","      <td>Positif</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Metdro2</td>\n","      <td>@Yatie84991237 @_TNIAL_ @edhi_shi @JohnVirgoVj...</td>\n","      <td>Negatif</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>bn_1c</td>\n","      <td>President Jokowi is Committed to Resolving Hum...</td>\n","      <td>Positif</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>adi27setiawan</td>\n","      <td>@jokowi Telek pak2, kekayaan indo hanya dinikm...</td>\n","      <td>Negatif</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>AyahFerGie</td>\n","      <td>@yosnggarang @jokowi Tiada hari tanpa omong ko...</td>\n","      <td>Negatif</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>de_fatah</td>\n","      <td>@JSuryoP1 Kasihan Pak Prabowo juga. Sudah rela...</td>\n","      <td>Negatif</td>\n","    </tr>\n","    <tr>\n","      <th>80</th>\n","      <td>mofaisalakbar</td>\n","      <td>@fadjarkim_ Itu orang pssi mirip jokowi, pante...</td>\n","      <td>Negatif</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>81 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fadd632-9aad-439f-9a10-860ea9a3b0c7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0fadd632-9aad-439f-9a10-860ea9a3b0c7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0fadd632-9aad-439f-9a10-860ea9a3b0c7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":136}],"source":["import pandas as pd\n","pd.options.mode.chained_assignment = None\n","# pd.options.display.max_colwidth = None\n","# pd.options.display.max_columns = None\n","# pd.options.display.max_rows = None\n","df = pd.read_csv('19_121_tweets_labeled.csv', usecols=['User', 'Tweet', 'Label'])\n","df"]},{"cell_type":"markdown","metadata":{"id":"Gf01LeezluH-"},"source":["# Preprocessing Data Tweets"]},{"cell_type":"markdown","metadata":{"id":"8IKjFjvlygc0"},"source":["## Case Folding\n","Case folding merupakan tahap text preprocessing yang berguna untuk :\n","\n","\n","*   Mengubah huruf kapital menjadi huruf kecil\n","*   Mengapus tanda baca\n","*   Menghapus angka\n","*   Menghapus karakter kosong\n","\n","<br>\n","\n","Sebelum melakukan case folding diatas, kode ini juga melakukan penghapusan link dan mention yang ada pada data tweet.\n","\n"]},{"cell_type":"code","execution_count":137,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664128984285,"user":{"displayName":"Adit Smile","userId":"14198219305169166491"},"user_tz":-420},"id":"DdsRluO5y7BB","outputId":"2cde36a3-60a0-454c-8644-68051232f7b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'bjorka apa anda jokowi butuh bantuan saya untuk menyelesaikan masalah ini pak '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":137}],"source":["import string\n","\n","#Mengapus link dan mention\n","df['Tweet'] = df['Tweet'].replace(r'\\s+',' ', regex=True)\n","indx = 0\n","for i in df['Tweet']:\n","  temp = df['Tweet'][indx].split()\n","  for j in temp:\n","    if 'http' in j:\n","      df['Tweet'] = df['Tweet'].replace(r'%s'%j,\" \", regex=True)\n","    if '@' in j:\n","      df['Tweet'] = df['Tweet'].replace(r'%s'%j,\" \", regex=True)\n","  indx+=1\n","\n","\n","\n","#mengubah menjadi huruf kecil\n","df['Tweet'] = df['Tweet'].str.lower()\n","\n","#menghapus tanda baca\n","for char in string.punctuation:\n","    df['Tweet'] = df['Tweet'].replace(r'[\\%s]'%char,\" \", regex=True)\n","\n","#menghapus angka\n","df['Tweet'] = df['Tweet'].replace(r'\\d+',' ', regex=True)\n","\n","#menghapus karakter kosong\n","df['Tweet'] = df['Tweet'].replace(r'\\s+',' ', regex=True)\n","df['Tweet'][2]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oiV0UFXuzDCE"},"source":["## Stopwords Removal\n","StopWords Removal merupakan tahap prepocessing yang berguna untuk menhapus kata umum yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Contoh stopword dalam bahasa Indonesia adalah “yang”, “dan”, “di”, “dari”, dll.\n"]},{"cell_type":"code","execution_count":138,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":571,"status":"ok","timestamp":1664128984845,"user":{"displayName":"Adit Smile","userId":"14198219305169166491"},"user_tz":-420},"id":"L0XKSgP5zGIt","outputId":"018be1e5-bc14-4d3f-c61d-15a96ee1d762"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'bjorka jokowi butuh bantuan menyelesaikan '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":138}],"source":["import urllib.request, json \n","with urllib.request.urlopen(\"https://raw.githubusercontent.com/smilesense/stopwords-id/master/stopwords-id.json\") as list_stopwords:\n","    data_stopword = json.load(list_stopwords)\n","\n","for i in data_stopword:\n","    df['Tweet'] = df['Tweet'].replace(r'\\b%s\\b'%i, '', regex=True)\n","df['Tweet'] = df['Tweet'].replace(r'\\s+',' ', regex=True)\n","\n","df['Tweet'][2]"]},{"cell_type":"markdown","metadata":{"id":"jC0wBzDDx7I8"},"source":[]},{"cell_type":"markdown","metadata":{"id":"3e-66tPOzHu2"},"source":["## Stemming\n","Stemming merupakan tahapan dalam preprocessing yang berguna untuk mengubah kata yang memiliki imbuhan menjadi kata dasarnya. Contohnya adalah, misal pada suatu teks terdapat kata \"berjalan\", maka dengan menerapkan metode stemming, teks tersebut akan diubah menjadi kata \"jalan\" atau bentuk dasar dari kata \"berjalan\". "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_24YMbOozLzf"},"outputs":[],"source":["# import StemmerFactory class\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","\n","# create stemmer\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","# stemming process\n","\n","ind = 0\n","for sentence in df['Tweet']:\n","    df['Tweet'][ind] = stemmer.stem(str(sentence))\n","    ind+=1\n"]},{"cell_type":"code","source":["df['Tweet'][2]"],"metadata":{"id":"t59jVufrzw5V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIRiBduLbivY"},"source":["## Tokenisasi\n","\n","Tokenizing adalah tahap preprocessing yang berguna untuk memisahkan teks menjadi potongan-potongan yang disebut sebagai token untuk kemudian di analisa. Kata, angka, simbol, tanda baca dan entitas penting lainnya dapat dianggap sebagai token."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8POvyWEbn7B"},"outputs":[],"source":["indeks = 0\n","df2 = df.copy()\n","for tweet in df2['Tweet']:\n","    df2['Tweet'][indeks] = str(tweet).split()\n","    indeks+=1\n","\n","df2['Tweet'][2]\n"]},{"cell_type":"markdown","metadata":{"id":"f-u6uo3fakBT"},"source":["## Hasil Preprocessing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgQGvyQWaqET"},"outputs":[],"source":["df2"]},{"cell_type":"markdown","metadata":{"id":"x9lxKxFRzWHA"},"source":["# Term Frequency (TF)\n","Term Frequency merupakan metode yang digunakan untuk mengetahui seberapa sering suatu kata muncul dalam suatu teks. Semakin banyak frekuensi kemunculan dari kata tsb, semakin besar pula nanti nilainya.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fLAU8J45zWrl"},"source":["## Term Frequency Keseluruhan"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3OnA5fGzRUb"},"outputs":[],"source":["listkata = []\n","for tweet in df['Tweet']:\n","    listkata = listkata + str(tweet).split()\n","\n","listkata_nodup = list(dict.fromkeys(listkata))\n","hasil_hitung = {}\n","for test1 in listkata_nodup:\n","    jumlah = 0\n","    for test2 in range(len(listkata)):\n","        if test1 == listkata[test2]:\n","            jumlah+=1\n","    hasil_hitung.update({'%s'%test1 : jumlah})\n","    \n","hasil_hitung = dict(sorted(hasil_hitung.items(), key=lambda item: item[1], reverse=True))\n","print(hasil_hitung)"]},{"cell_type":"markdown","metadata":{"id":"03oQY4bDzeLq"},"source":["## Term Frequency Tiap Tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9qPmBkdj_2C"},"outputs":[],"source":["def terms(dataframe):\n","  # pd.options.mode.chained_assignment = None\n","  # pd.options.display.max_colwidth = None\n","  # pd.options.display.max_columns = None\n","  # pd.options.display.max_rows = None\n","\n","  df3 = dataframe.copy()\n","  a = 1\n","  for inter in range(len(df3['Tweet'])):\n","      for fitur in hasil_hitung:\n","          df3['%s'%(fitur)] = 0\n","          a+=1\n","\n","  for inter in range(len(df3['Tweet'])):\n","    for fitur in hasil_hitung:\n","        cek = df3['Tweet'][inter]\n","        jumlah = 0\n","        for iter2 in range(len(cek)):\n","          if fitur == cek[iter2]:\n","            jumlah+=1\n","        df3['%s'%(fitur)][inter] = jumlah\n","        a+=1\n","  df3.to_csv(r'my_data.csv', index=False)\n","  return df3\n","  \n","\n","terms(df2)"]},{"cell_type":"markdown","source":["# Seleksi Fitur Menggunakan Information Gain (Mutual Information)\n","\n","Information Gain Merupakan metode yang digunakan untuk mengukur seberapa penting / berpengaruh sebuah fitur terhadap hasil pengukuran. Penggunaan teknik ini dapat mereduksi dimensi fitur dengan cara mengukur reduksi Entropy sebelum dan sesudah pemisahan. Metode ini juga dikenal dengan sebutan Mutual Information, biasanya digunakan untuk mengetahui dependency antara dua variable \"x\" dan \"y\".\n"],"metadata":{"id":"tbb3Bf-aWjO_"}},{"cell_type":"markdown","source":["## Melakukan Training \n","Training disini berfungsi untuk menghindari overfitting, yaitu keadaan dimana suatu model berusaha untuk mempelajari seluruh detail dalam data."],"metadata":{"id":"0nrob7ncuLjS"}},{"cell_type":"code","source":["df4 = pd.read_csv('my_data.csv')\n","df4['Label'].unique()"],"metadata":{"id":"LEQPnQlcKxzb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(df4.drop(labels=['Label', 'User', 'Tweet'], axis=1),\n","    df4['Label'],\n","    test_size=0.3,\n","    random_state=0)\n","y_train"],"metadata":{"id":"ZzFGv6ZyWi1g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Melakukan  (KNN)"],"metadata":{"id":"CnmMv-hBrpjC"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","list_y_pred = []\n","for i in range(2,len(y_train)):\n","  neigh = KNeighborsClassifier(n_neighbors=i)\n","  neigh.fit(X_train, y_train)\n","  y_pred = neigh.predict(X_test)\n","  list_y_pred2 = [i, y_pred] \n","  list_y_pred.append(list_y_pred2)\n","list_y_pred"],"metadata":{"id":"-KXCHd23YQ5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import make_scorer, accuracy_score,precision_score\n","list_akurasi = []\n","for i in range(len(list_y_pred)):\n","  # print(list_pred[i])\n","  # testing = neigh.predict(X_test) \n","  accuracy_neigh=round(accuracy_score(y_test,list_y_pred[i][1])* 100, 2)\n","  acc_neigh = round(neigh.score(X_train, y_train) * 100, 2)\n","  list_akurasi2 = [list_y_pred[i][0], accuracy_neigh]\n","  list_akurasi.append(list_akurasi2)\n","#list_akurasi\n","df5 = pd.DataFrame(list_akurasi, columns=[\"Nearest Neighbors\",\"Akurasi\"])\n","df5"],"metadata":{"id":"hAqIv26Ibwr-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Melakukan seleksi Fitur\n","Setelah melakukan training untuk menghindari overfitting, selanjutnya data hasil training tersebut akan dilakukan feature selection menggunakan metode mutual information classification"],"metadata":{"id":"gY_4ooI8vZB6"}},{"cell_type":"code","source":["from sklearn.feature_selection import mutual_info_classif\n","mutual_info = mutual_info_classif(X_train, y_train)\n","mutual_info"],"metadata":{"id":"1__RcBJpot9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Melakukan Ranking Fitur\n","Setelah selesai melakukan seleksi fitur, pada tahap ini kita akan meranking fitur tersebut berdasarkan tingkatan information gain (mutual information) yang dimiliki."],"metadata":{"id":"qV_VaohNpbTV"}},{"cell_type":"code","source":["mutual_info = pd.Series(mutual_info)\n","mutual_info.index = X_train.columns\n","mutual_info.sort_values(ascending=False)"],"metadata":{"id":"4orZlRA9o2lE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#let's plot the ordered mutual_info values per feature\n","mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))"],"metadata":{"id":"L19t_vE8pATv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_selection import SelectKBest\n","\n","sel_best = SelectKBest(mutual_info_classif, k=50)\n","sel_best.fit(X_train, y_train)\n","X_train.columns[sel_best.get_support()]"],"metadata":{"id":"szNpB_6_8lwG"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}